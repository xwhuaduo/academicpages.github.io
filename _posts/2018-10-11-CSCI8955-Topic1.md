---
title: 'Sparse coding and dictionary learning Summary'
date: 2018-10-11
permalink: /posts/2018/10/blog-CSCI8955-Topic1/
tags:
  - Sparse coding
  - Dictionary learning
  - Summary
---

1. Sparse coding and dictionary learning
=====================================

Sparse Dictionary Learning is a method which aims at finding a sparse
representation of the inpute data in the form of a linear combination of
basic elements as well as those basic elements themselves. It consists
of two parts, sparse coding and dictionary learning.

1.1 Definition
----------

Let $\mathbf{x_1},\mathbf{x_2},...,\mathbf{x_n} \in R^d$ be all the $n$ known
samples and matrix $\mathbf{X} \in R^{d\times n} (d<n)$, which is
constructed by known samples, is the measurement matrix or the basis
dictionary. Each column of $\mathbf{X}$ is one sample and the probe sample
is $\mathbf{y} \in R^d$, which is a column vector. Thus if all the known
samples are used to approximately represent the probe sample, it should
be express as:

$$\mathbf{y} = \mathbf{x_1}\alpha_1+\mathbf{x_2}\alpha_2+...+\mathbf{x_n}\alpha_n$$

where $\alpha_i ~(i=1,2,...,n)$ is the coefficient of $x_i$. Eq. (1) can
be rewritten as $\mathbf{y}=X\mathbf{\alpha}$, where matrix
$\mathbf{X}=[\mathbf{x_1},\mathbf{x_2},...,\mathbf{x_n}]$ and
$\mathbf{\alpha}=[\alpha_1,\alpha_2,...,\alpha_n]^T$. However, it is
impossible to utilize this equation to uniquely represent the probe
sample $\mathbf{y}$ using the measurement matrix $\mathbf{X}$. We add a
constrain, $\phi(\mathbf{\alpha})\leq \epsilon$, to find a representation
solution $\mathbf{\alpha}$. The penalty term $\phi(\mathbf{\alpha})$ and the
threshold $\epsilon$ are chosen such that the solution
$\hat{\mathbf{\alpha}}$ is a *sparse* vector. A “Sparse" vector means that
some elements of the vector are zero. The problem Eq.(1) is converted to
the following optimization problem:

$$\mathbf{y}=\mathbf{X}\mathbf{\alpha} ~~~~~~ s.t.~~~~~~  \phi(\mathbf{\alpha})\leq \epsilon,$$

or equivalently,

$$\hat{\mathbf{\alpha}}=\underset{\mathbf{\alpha}}{argmin} ||\mathbf{y}-\mathbf{X}\mathbf{\alpha}||_2^2 +\lambda\phi(\mathbf{\alpha}),$$

for some $\lambda$. The procedure of finding $\hat{\mathbf{\alpha}}$ is
called *Sparse Coding*. Sometimes we don’t have the probe sample
$\mathbf{y}$ but only $\mathbf{X}$. We need to find a matrix $\mathbf{D}$ such that
any sample can be approximately represented by a linear combination of
columns of $\mathbf{D}$, and a matrix $\mathbf{A}$, whose columns are sparse
vectors, i.e.

$$\underset{\mathbf{D,A}}{min} ~~||\mathbf{X}-\mathbf{DA}||_F^2 +\lambda\Phi(\mathbf{A}),$$

The process of finding $\mathbf{D}$ and $\mathbf{A}$ is (unsupervised)
*Dictionary Learning*, and $\mathbf{D}$ is called the *Dictionary*.

For $\phi(.)$, the most straightforward form is $l_0$-norm, which makes
Eq. (3) a non-deterministic polynomial-time hard (NP-hard) problem.
Recent literatures have demonstrated that when the solution obtained by
using the $l_1$-norm minimization constraint is also content with the
condition of sparsity, the solution using $l_1$-norm minimization with
sufficient sparsity can be equivalent to the solution obtained by
$l_0$-norm minimization with full probability. Other forms of $\phi(.)$
may also be taken into consideration, such as $l_p$-norm $(0<p<1)$,
$l_{2,1}$ norm, which is also called the rotation invariant $l_1$-norm,
and $l_2$-norm, which only lead to “limitedly-sparse.“

1.2 Optimization
------------

There are four groups of methods to solve Eq. (2) and (3), and several
new methods are created to solve Eq. (4), based on certain algorithms
from these four groups of methods or not.

### 1.2.1 Greedy strategy approximation

The greedy strategy approximation is mainly used to provide an
approximate solution to $l_0$-norm minimization problems. The greedy
strategy searches for the best local optimal solution in each iteration
with the goal of achieving the optimal global solution. This methods
contain *Matching Pursuit* (MP) Algorithm. Based on MP, the *Orthogonal
Matching Pursuit*(OMP) Algorithm employs the process of
orthogonalization to guarantee the orthogonal direction of projection in
each iteration, which guarantees to provide a *k-sparsity* solution in
$k$ iterations. New algorithms based on MP or OMP are created to make an
improvement or to deal with certain kind of data.

### 1.2.2 Constrained optimization strategy

Constrained optimization strategy is utilized to obtain the solution of
sparse representation with the $l_1$-norm regularization. The methods
address the non-differentiable unconstrained problem by reformulating it
as a smooth differentiable constrained optimization problem. This
strategy exploits the constrained optimization method with efficient
convergence to obtain the sparse solution. *Gradient Projection Sparse
Reconstruction* (GPSR), *Interior-point method based sparse
representationstrategy* and *Alternating direction method (ADM) based
sparse representation strategy* are created based on this idea.

### 1.2.3 Proximity algorithm based optimization strategy

This strategy reformulates the original problem into the specific model
of the corresponding proximal operator such as the soft thresholding
operator, hard thresholding operator, and resolvent operator, and then
exploits the proximity algorithms to address the original sparse
optimization problem. This strategy includes *Augmented Lagrange
Multiplier based optimization strategy*, *Coordinate Descent for the
Lasso*, etc.

### 1.2.4 Homotopy algorithm based strategy

This strategy iteratively traces the final desired solution starting
from the initial point to the optimal point by successively adjusting
the homotopy parameter. The homotopy algorithm is used to solve the
$l_1$-norm minimization problem with $k$-sparse property.

### 1.2.5 Methods for Dictionary Learning

Based on the idea of *Alternate Minimization*, *Method of Optimal
Directions (MOD)* is created from OMP. The only modification is that
after we compute the sparse code using OMP ( $\mathbf{D}$ is given an
initial value), we update $\mathbf{D}$, and iterate this whole procedure $t$
times to get $\hat{\mathbf{D}}$ and $\hat{\mathbf{A}}$. Another method is to
updates the dictionary with *Block Coordinate Decent* after we compute
the sparse code with coordinate descent algorithm for the Lasso. As a
modification of MOD, *K-SVD* was created and becomes the most
representative unsupervised dictionary learning. *Online Dictionary
Learning* is created to handle large and dynamic dataset.

2. Low-rank Approximation and Optimization
=======================================

This question concerns finding a low-rank representation that captures
as much of the information as possible from the high-dimensional data,
i.e. $\mathbf{D}=\mathbf{X}+\mathbf{E}$, where $\mathbf{D}$ is the data matrix, $\mathbf{X}$
is the *low-rank* component; $\mathbf{E}$ is the noise or error
measurements. The goal is find $\hat{\mathbf{X}}$ that minimizes

$$||\mathbf{D}-\mathbf{X}||_F^2~~s.t. ~~ rank(\mathbf{X})\leq r.$$
 Specifically,
there are two purposes of low-rank approximation and optimization, i.e.
*Matrix Completion* and *Matrix Recovery*. Matrix completion aims at
recovering a matrix $\mathbf{X}$ of the lowest possible rank that agrees
with $\mathbf{D}$ on all the observed pixels. Matrix recovery aims at
recovering some observations that are grossly corrupted.

2.1 Definition
----------

The assumption of Matrix Completion is that the data should be
low-dimensional (low-rank). The corresponding problem formulation is

$$min ~rank(\mathbf{X}), ~s.t. \mathbf{X}_{ij} = \mathbf{D}_{ij}, \forall(i,j)\in \Omega.$$

Since this is a NP-hard problem, we switch to minimizing the nuclear
norm, $||\mathbf{X}||_*$, which is a convex envelope of $rank(\mathbf{X})$. The
nuclear norm of $\mathbf{X}$ equals the sum of non-zero eigenvalues of
$\mathbf{X}$. Also, we use the linear operator form, restricting the
equality only on the entries in $\Omega$. Now, the convex surrogate
version of the formulation is,

$$min ~||\mathbf{X}||_*, ~s.t. P_{\Omega}(\mathbf{X}) = P_{\Omega}(\mathbf{D})$$

The assumption of Matrix Recovery is that the data has a low-rank
structure with sparse errors. The goal is that given $D=X+E$, recovering
$\mathbf{X}$ and $E$, i.e.

$$min ~rank(\mathbf{X})+\gamma||\mathbf{E}||_0, ~s.t. \mathbf{X}+\mathbf{E}=\mathbf{D},$$

where $||\mathbf{E}||_0$ equals to the number of non-zero entries in it.
Still, this is a NP-hard problem. We use the following convex surrogate
version instead,

$$min ~||\mathbf{X}||_*+\gamma||\mathbf{E}||_1, ~s.t. \mathbf{X}+\mathbf{E}=\mathbf{D},$$

Matrix completion could be seen as a special case of matrix recovery
since it essentially recovers $\mathbf{X}$ from a few entries, $$min ~||\mathbf{X}||_*, ~s.t. P_{\Omega}(\mathbf{X}) = P_{\Omega}(\mathbf{D})$$. While matrix recovery is to recover $\mathbf{X}$ from gross errors, $$min ~||\mathbf{X}||_*+\gamma||\mathbf{E}||_1, ~s.t. \mathbf{X}=\mathbf{D}-\mathbf{E}$$.

2.2 Approximation and Optimization
------------------------------

*PCA* could be used to get the best $r$-rank approximation, but it
breaks down under even a single corrupted observation. *Singular Value
Thresholding* could approximately solve the nuclear norm minimization
problems to get the solution for matrix completion. For matrix recovery
problems, we could use *Robust PCA via Iterative Thresholding*, which is
based on *Alternating Direction Method of Multipliers* (ADMM), to solve
convex optimization problems by breaking them into smaller pieces that
are easier to handle. Other methods include the *Augmented Lagrange
Multiplier (ALM) for RPCA*, etc.

3. Applications
============

Sparse coding and dictionary learning have been successfully implemented
to numerous applications, especially in the fields of computer vision,
image processing, pattern recognition, and machine learning, etc.
Low-rank approximation and Optimization had been implemented for video
backgrounds modeling, image classification and registration, etc.
