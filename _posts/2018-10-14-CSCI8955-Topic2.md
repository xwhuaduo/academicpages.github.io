---
title: 'Random Forest Summary'
date: 2018-10-14
permalink: /posts/2018/10/blog-CSCI8955-Topic2/
tags:
  - Random Forest
  - Machine Learning
  - Summary
---

Introduction
============

Random forest (RF) is an ensemble learning method for classification,
regression, density estimation, manifold learning, etc. It builds a
large collection of de-correlated trees, and then ensembles them. RF is
simpler to train and tune, and it performs similarly compared to
boosting on many problems, which leads to the popularity of it.

Tree-based methods
==================

RF is a tree-based method, which partitions the feature space into a set
of distinct and non-overlapping regions, and then fit a simple model
(such as a constant for regression problem) in each one. We illustrate
several fundamental methods here.

Decision Trees
--------------

Decision trees (DTs) are a non-parametric supervised learning method
used for classification and regression. DTs use a *recursive binary
splitting* strategy to grow a tree, perform a *greedy algorithm* at each
splitting, and stop growing the tree when certain criteria or threshold
is met. We define the resulting splitted region as a terminal (leaf)
node, the root node as the first split, define internal nodes as the
places where the rest splits happen.

For DTs, complicated cases are simplified to *recursive binary*
partitions, which is good for interpretability. Also, the reason we
choose binary splits is that multiway splits fragment the data too
quickly, leaving insufficient data at the next level down. Besides, one
can see that any $n$-ary tree can be transformed into an equivalent
binary tree.

By implementing the *greedy algorithm* at each splitting, we choose the
split that could minimize the objective function, which measures the
node impurity.

There are several methods to define the objective function. For
categorical response $Y$ taking values $1,2,...,K$. Let $|T|$ denote the
number of terminal nodes in $T$. In node $m$, representing a region
$R_m$ with $N_m$ observations, let $\hat{p}_{mk}=\frac{1}{N_m}\Sigma_{x_i \in R_m}I(y_i=k),$ the
proportion of class $k$ observations in node $m$. We classify the
observations in node $m$ to class $k(m)=argmax_k ( \hat{p}_{mk})$, be
the majority class in node $m$. Different measures $Q_m(T)$ of node
impurity include the following:

**Misclassification error**:
$\frac{1}{N_m}\Sigma_{i\in R_m}I(y_i\neq k(m))=1-\hat{p}_{mk(m)}$

**Gini index**:
$\Sigma_{k\neq k'}\hat{p}_{mk}\hat{p}_{mk'}=\Sigma_{k=1}^{K}\hat{p}_{mk}(1-\hat{p}_{mk})$

**Shannon entropy**:$-\Sigma_{k=1}^{K}\hat{p}_{mk}log(\hat{p}_{mk})$

For continuous response $Y$, we define
$\hat{c}_m=\frac{1}{N_m}\Sigma_{x_i\in R_m} y_i$, which is the mean
value of $Y$ in the $m$th region. The node impurity $Q_m(T)$ is defined
here,

**Mean Square Error (MSE)**:
$\frac{1}{N_m}\Sigma_{x_i\in R_m} (y_i-\hat{c}_m)^2$

**Differential entropy**: 
$-\int_{y \in \mathcal{Y}}p(y)log(p(y))dy$

Here $p$ is the probability density function estimated from the training
points in the region $R_m$. The distribution $p(y)$ can be defined
either using parametric distributions or non-parametric methods.

The tree depth is a tuning parameter governing the model’s complexity,
and the optimal tree depth should be adaptively chosen from the data.
The preferred strategy is to grow a large tree $T_0$, stopping the
splitting process only when some minimum node size (say 5) is reached.
Then this large tree is pruned using *cost-complexity pruning*.

We define the cost complexity criterion here,
$$C_{\alpha}(T)=\Sigma_{m=1}^{|T|}N_mQ_m(T)+\alpha|T|$$

The tuning parameter $\alpha$ controls a trade-off between the subtree’s
complexity and goodness of fit to the training data. We consider a
sequence of trees indexed by a nonnegative tuning parameter $\alpha$.
For each value of $\alpha$, there corresponds a subtree $ T_0 \subset T$
such that Eq. (1) is minimized. We use cross-validation to select
$\alpha$ and get the subtree corresponding to the chosen value of
$\alpha$.

Typically, for categorical cases, the misclassification rate is used to
guide cost-complexity pruning, while Gini index and cross-entropy are
used when growing the tree.

Bagging
-------

The hierarchical nature of the process of growing the tree makes it come
with high variance. Bagging averages many trees to reduce this variance.
To apply bagging to regression trees, we simply construct $B$ regression
trees using $B$ bootstrapped training sets and average the resulting
predictions. These trees are grown deep and are not pruned. Hence each
individual tree has high variance, but low bias. Averaging these $B$
trees reduces the variance. For classification trees, we can record the
class predicted by each of the $B$ trees, and take a majority vote: the
overall prediction is the most commonly occurring majority class among
the $B$ predictions. It is worth noting that $B$ is not a critical
parameter with bagging, and using a very large value of $B$ will not
lead to overfitting.

Random forest
=============

Suppose there is one strong variable, most or all of the trees will use
this strong predictor in the top split. The predictions from the bagged
trees will be highly correlated. Averaging many highly correlated
quantities does not lead to as large of a reduction in variance as
averaging many uncorrelated quantities. RF overcomes this problem by
forcing each split to consider only a subset of the predictors,
decorrelating the trees, thereby making the average of the resulting
trees less variable and hence more reliable.

Definition
----------

To define RF in a unified fashion, we redefine the notations here.
Denote $\mathbf{S}_j$ as the dataset of the current node, $\mathbf{S}_j^L$ and
$\mathbf{S}_j^R$ as the datasets after splitting from $\mathbf{S}_j$. Denote the
split function (weak learner) at node $j$ as
$h(\mathbf{v},\mathbf{\theta}_j): \mathbf{R}^d \times \mathcal{T} \to \{0,1\} $,
where 0 and 1 can be interpreted as “false" and “true", respectively,
$\mathbf{v}$ represents a training point, $\mathbf{\theta}_j \in \mathcal{T}$
denote the split parameters associated with the $j$th node, and
$\mathcal{T}$ represents the space of all possible split parameters.

Denote the objective function, information gain:

$$I = H(\mathbf{S})-\Sigma_{i\in\{L,R\}}\frac{|\mathbf{S}^i|}{|\mathbf{S}|}H(\mathbf{S}^i),$$

where $H(\mathbf{S})$ is the entropy of set $\mathbf{S}$, which measures the
node impurity, $|\mathbf{S}|$ represents the number of point in set
$\mathbf{S}$. At each node, we find the best split, represented by the
parameter $\mathbf{\theta}_j$ where

$$\mathbf{\theta}_j=\underset{\mathbf{\theta} \in \mathcal{T} }{argmin}  I(\mathbf{S}_j,\mathbf{\theta})$$

where
$\mathbf{S}_j^L(\mathbf{S}_j,\mathbf{\theta})=\{(\mathbf{v},.)\in \mathbf{S}_j | h(\mathbf{v},\mathbf{\theta})=0\}$,
$\mathbf{S}_j^R(\mathbf{S}_j,\mathbf{\theta})=\{(\mathbf{v},.)\in \mathbf{S}_j | h(\mathbf{v},\mathbf{\theta})=1\}.$

Specifically, we denote the parameters of the weak learner as
$\mathbf{\theta}=(\mathbf{\phi},\mathbf{\psi},\mathbf{\tau})$ where the filter function
$\mathbf{\phi}(\textbf{v})$ selects some features of choice out of the
entire vector $\textbf{v}$; $\mathbf{\psi}$ defines the geometric primitive
used to separate the data (e.g., an axis-aligned hyperplane, an oblique
hyperplane, or a general surface); and the parameter vector $\mathbf{\tau}$
captures thresholds for the inequalities used in the binary test.

Linear data separation (such as, axis-aligned hyperplane and oblique
hyperplan) could be represented by
$h(\mathbf{v},\mathbf{\theta})=[\tau_1>\mathbf{\phi(\mathbf{v})}\cdot \mathbf{\psi}>\tau_2]$.
Non-linear data separation (such as quadratic surface) could be
represented by
$h(\mathbf{v},\mathbf{\theta})=[\tau_1>\mathbf{\phi^T(\mathbf{v})} \cdot \mathbf{\psi} \cdot \mathbf{\phi(\mathbf{v})}>\tau_2]$.
We could set $\tau_1=\infty$ or $\tau_2=-\infty$ to get an binary split.

The *Leaf Prediction Models* are responsible for providing a good
prediction after we optimize weak learners and tree structures. In the
most general sense the leaf statistics can be captured using the
conditional distributions $p(c|\mathbf{v})$ or $p(\mathbf{y}|\mathbf{v})$, where $c$
and $\mathbf{y}$ represent the categorical and continuous lables. Different
leaf predictors can be used, for instance, a Maximum A-Posterior (MAP)
estimate which is obtained by $c^*=\underset{c}{argmax}  ~p(c|\mathbf{v})$
or $\mathbf{y}^*=\underset{\mathbf{y}}{argmax}  ~p(\mathbf{y}|\mathbf{v})$ for the
categorical and continuous cases, respectively. However, in general it
is preferable to keep the entire distribution around until the final
moment where a decision must be taken, rather than taking an early point
estimate. This allows us to reason about prediction uncertainty.

Randomness
----------

Two of the most popular ways to inject randomness into the trees during
the training phase are *random training set sampling* (e.g., bagging)
and *randomized node optimization (RNO)*. The key for fitting RF model
is the second one, while one can choose to combine with bagging or not.
The pro and con of bagging come from the fact that it uses only a subset
of the training data to fit each tree, which seems wasteful but makes
training faster. Besides, avoiding using bagging is helpful for getting
the maximum margin of RF when doing classification.

In RNO, we train the $j$th node on a small random subset
$\mathcal{T}_j \subset \mathcal{T}$ of parameter values. By this way, we
could de-correlate each tree with more efficiency.

$$\mathbf{\theta}_j=\underset{\mathbf{\theta\in \mathcal{T}_j  } }{argmin}  I(S_j,\mathbf{\theta})$$

Ensembling methods
------------------

Averaging operation or the product of all tree posteriors could be
implemented to combine trees into a forest ensemble. One should be
careful when using the product since it makes the model less robust to
noise.

Key Model Parameters
--------------------

**Forest size $T$**: increasing $T$ will not lead to overfitting. As $T$
increases, the posteriors would be smoother, which is good from the view
of generalization. **Tree depth $D$**: we can see that as $D$ increases,
the probability of overfitting increases as well. Large amounts of
training data are needed to fit RF with deep trees. **Randomness
$\rho=|\mathcal{T}_j|$**: with smaller randomness (large $\rho$), trees
fitted will have similar (high correlation) structure and output,
ensembling them would yield posteriors with higher overall confidence.
With large randomness (smaller $\rho$), trees fitted will not likely to
have similar structure and output, ensembling them would yield
posteriors with lower overall confidence.
