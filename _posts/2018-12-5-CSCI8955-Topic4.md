---
title: 'Image Registration Summary'
date: 2018-12-5
permalink: /posts/2018/12/blog-CSCI8955-Topic4/
tags:
  - Image Registration
  - Machine Learning
  - Summary
---

Introduction
============

Image registration is the process of overlaying two or more images of
the same scene taken at different times, from different viewpoints,
and/or by different sensors. It makes the pixels in two or more images
precisely coincide to the same points in the scene. Image registration
is a crucial step in all image analysis tasks in which the final
information is gained from the combination of various data sources like
in image fusion, change detection, and multichannel image restoration.
Typically, registration is required in remote sensing, in medicine, in
cartography, and in computer vision, etc.

Methodology of Image Registration
=================================

In general, its applications can be divided into four main groups
according to the manner of the image acquisition:

*Different viewpoints* (*multiview analysis*). Images of the same scene
are acquired from different viewpoints; *Different times*
(*multitemporal analysis*). Images of the same scene are acquired at
different times, often on a regular basis, and possibly under different
conditions; *Different sensors* (*multimodal analysis*). Images of the
same scene are acquired by different sensors; *Scene to model
registration*. Images of a scene and a model of the scene are
registered. The model can be a computer representation of the scene, for
instance, maps or digital elevation models (DEM) in GIS, another scene
with similar content (another patient), ‘average’ specimen, etc.

Due to the diversity of images to be registered and due to various types
of degradations, it is impossible to design a universal method
applicable to all registration tasks. Every method should take into
account not only the assumed type of geometric deformation between the
images but also radiometric deformations and noise corruption, required
registration accuracy and application-dependent data characteristics.
Nevertheless, the majority of the registration methods consists of the
following four steps: *feature detection*, *feature matching*,
*transform model estimation*, *Image resampling and transformation*.

Feature detection
=================

Salient and distinctive objects are manually or, preferably,
automatically detected. For further processing, these features can be
represented by their point representatives , which are called control
points (CPs) in the literature. The features should be distinctive
objects, which are frequently spread over the images and which are
easily detectable. Usually, the physical interpretability of the
features is demanded. In an ideal case, the algorithm should be able to
detect the same features in all projections of the scene regardless of
the particular image deformation.

Area-based methods
------------------

This approach works directly with image intensity values. Hence, it put
emphasis rather on the feature matching step than on their detection.
For this approach, the first step of image registration is omitted.

Feature-based methods
---------------------

This approach is based on the extraction of salient structures features
in the images. Significant regions, lines or points are understood as
features here. The features represent information on a higher level and
should be distinct, spread all over the image and efficiently detectable
in both images. They are expected to be stable in time to stay at fixed
positions during the whole experiment. The number of common elements of
the detected sets of features should be sufficiently high, regardless of
the change of image geometry, radiometric conditions, presence of
additive noise, and of changes in the scanned scene. This property makes
feature-based methods suitable for situations when illumination changes
are expected or multisensor analysis is demanded. Three kinds of
features are used in this step, they are region features, line features,
and point features.

Feature matching
================

In this step, the correspondence between the features detected in the
sensed image and those detected in the reference image is established.
Various feature descriptors and similarity measures along with spatial
relationships among the features are used for that purpose. The matching
algorithm in the space of invariants should be robust and efficient.
Single features without corresponding counterparts in the other image
should not affect its performance.

Area-based methods
------------------

Area-based methods merge the feature detection step with the matching
part. These methods deal with the images without attempting to detect
salient objects. Windows of a predefined size or even entire images are
used for the correspondence estimation during the second registration
step. The problems with this approach are that the comparability of
windows of predefined size is violated if more complicated geometric
deformations are present between images. Another disadvantage of the
area-based methods is that the windows may not contain distinctive parts
of the image. Several different sensors types are *correlation-like
methods*, textit[fourier methods]{}, *Mutual information methods*, and
*Optimization methods*.

Feature-based methods
---------------------

Assume that two sets of features in the reference and sensed images
represented by the CPs have been detected, the aim is to find the
pairwise correspondence between them using their spatial relations or
various descriptors of features. The methods used here include *methods
using spatial relations*, *methods using invariant descriptors* ,
*relaxation methods* , and *pyramids and wavelets*.

Area-based methods are preferably applied when the images have not many
prominent details and the distinctive information is provided by gray
levels/colors rather than by local shapes and structure. Feature-based
matching methods are typically applied when the local structural
information is more significant than the information carried by the
image intensities.

Transform model estimation.
===========================

The type and parameters of the so-called mapping functions, aligning the
sensed image with the reference image, are estimated. The parameters of
the mapping functions are computed by means of the established feature
correspondence. The type of the mapping functions should be chosen
according to the a priori known information about the acquisition
process and expected image degradations. If no a priori information is
available, the model should be flexible and general enough to handle all
possible degradations which might appear. After the feature
correspondence has been established the mapping function is constructed.
It should transform the sensed image to overlay it over the reference
one. In this step, the type of the mapping function and its parameter
estimation are chosen. Models of mapping functions can be divided into
two broad categories according to the amount of image data they use as
their support: *global mapping models*, and *local mapping models*.

From another point of view, mapping functions can be categorized
according to the accuracy of overlaying of the CPs used for computation
of the parameters. *Interpolating functions* map the sensed image CPs on
the reference image CPs exactly, whereas *approximating functions* try
to find the best trade-off between the accuracy of the final mapping and
other requirements imposed on the character of the mapping function.

Image resampling and transformation
===================================

The sensed image is transformed by means of the mapping functions. Image
values in non-integer coordinates are computed by the appropriate
interpolation technique. The choice of the appropriate type of
resampling technique depends on the trade-off between the demanded
accuracy of the interpolation and the computational complexity.

The mapping functions constructed during the previous step are used to
transform the sensed image and thus to register the images. The
transformation can be realized in a forward or backward manner. In the
forward method, each pixel from the sensed image can be directly
transformed using the estimated mapping functions, however, this
approach it can produce holes and/or overlaps in the output image.
Hence, the backward approach is usually chosen. The registered image
data from the sensed image are determined using the coordinates of the
target pixel and the inverse of the estimated mapping function. The
image interpolation takes place in the sensed image on the regular grid.
In this way, neither holes nor overlaps can occur in the output image.
The most commonly used interpolants include nearest neighbor function,
the bilinear, and bicubic functions, quadratic splines, cubic B-
splines, higher-order B-splines, Catmull?Rom cardinal splines,
Gaussians, and truncated sinc functions.

Evaluation of accuracy
======================

The accuracy evaluation is a non- trivial problem, since it is hard to
distinguish between registration inaccuracies and actual physical
differences in the image contents. Several methods used to finish the
job include: *Localization error* measures the displacement of the CP
coordinates due to their inaccurate detection. *Matching error* is
measured by the number of false matches when establishing the
correspondence between CP candidates. *Alignment error* is always
present in practice because of two different reasons. The type of the
chosen mapping model may not correspond to the actual distortion and/or
the parameters of the model were not calculated precisely. Estimation of
the accuracy of registration algorithms is a substantial part of the
registration process. Without quantitative evaluation, no registration
method can be accepted for practical utilization.
