---
title: 'Deep Learning Summary'
date: 2018-11-15
permalink: /posts/2018/11/blog-CSCI8955-Topic3/
tags:
  - Deep Learning
  - CNN
  - RNN
  - Machine Learning
  - Summary
---

1. Introduction
============

Deep learning allows computational models that are composed of multiple
processing layers to learn representations of data with multiple levels
of abstraction. These methods have dramatically improved the
state-of-the-art in speech recognition, visual object recognition,
object detection and many other domains such as drug discovery and
genomics. Deep learning discovers intricate structure in large datasets
by using the backpropagation algorithm to indicate how a machine should
change its internal parameters that are used to compute the
representation in each layer from the representation in the previous
layer. Deep convolutional nets have brought about breakthroughs in
processing images, video, speech, and audio, whereas recurrent nets have
shone light on sequential data such as text and speech.

2. Deep Feedforward Networks
=========================

Deep Feedforward Networks (DFNs) are the quintessential deep learning
models. The goal of a feedforward network is to approximate some
function $f^*$.

2.1 Definition
----------

DFNs are called **feedforeward** because information flows through the
function being evaluated from $\mathbf{x}$, through the intermediate
computations used to define $f$, and finally to the output $\mathbf{y}$. The
model will have three types of layers, the input layer, hidden layer(s)
and the output layer. The layers consist of neurons. A neuron receives
input from neurons of the previous layer and computes its own activation
value. The overall number of layers defines the depth of DFNs. These
networks are called neural because they are loosely inspired by
neuroscience. It is best to think of DFNs as function approximation
machines that are designed to achieve statistical generalization rather
than as models of brain function.

2.2 Activation Functions
--------------------

DFNs could be seen as linear transformation functions of the input
variables if without activation function implementing non-linear
transformation on each neuron. Activation Functions ensure the
explanation power of DFNs. The most commonly used activation functions
include sigmoid ($S(x) = 1/(1+e^{-x})$, tanh ($tanh(x)$), ReLU
($max(0,x)$), Leaky ReLU ($max(0.1x,x)$), etc. Sigmoid perform better
when neural networks are very small, but it has gradient vanishing
problems as we increase or decrease $x$. The gradient of the ReLU does
not vanish as we increase $x$. ReLU also induces the sparsity in the
hidden units.

2.3 Optimization
------------

We compute an objective function that measures the error between $f^* $
and our approximation of $f^* $. The learning algorithm makes the model
modifies its internal adjustable parameters (weight vector) to reduce
this error. To properly adjust the weight vector, the learning algorithm
computes a gradient vector that, for each weight, indicates by what
amount the error would increase or decrease if the weight were increased
by a tiny amount. The weight vector is then adjusted in the opposite
direction to the gradient vector.

The objective function can be seen as a kind of hilly landscape in the
high-dimensional space of weight values. The negative gradient vector
indicates the direction of steepest descent in this landscape, taking it
closer to a minimum. The Vanilla Gradient Descent method uses the entire
training set to calculate the gradient, while Stochastic Gradient
Descent (SGD) method use a single observation or a small subset of
observations at a time. Many improvements of SGD are created to shorten
the learning procedure.

Algorithms that adjust the gradient direction include Momentum and
Nesterov Momentum. Momentum accumulates an exponentially decaying moving
average of past gradients and continues to move in their direction.
Nesterov Momentum method makes improvement by adding a correction
factor. Algorithms with adaptive learning rates include AdaGrad,
RMSProp, Adam, etc. AdaGrad individually adapts the learning rates of
all model parameters by scaling them inversely proportional to the
square root of the sum of all the historical squared values of the
gradient. RSMProp modifies AdaGrad to perform better in the nonconvex
setting by changing the gradient accumulation into an exponentially
weighted moving average. Adam could be seen as a variant on the
combination of RMSProp and Momentum with a few important distinctions.
In Adam, momentum is incorporated directly as an estimate of the
first-order moment of the gradient, and bias corrections are included to
the estimates of both the first-order moments and the second-order
moments to account for their initialization at the origin.

2.4 Regularization
--------------

After training, the performance of the system is measured on a different
set of examples called a test set to test the generalization ability of
the model. Regularization is modifications that are made to a learning
algorithm to increase its generalization ability. There are many
Regularization methods. L1 Regularization leads the weight vectors to
become sparse during optimization, L2 regularization and Max Norm
Constraints put penalties and set upper bound to weight vectors,
respectively. Data Augmentation creates more data to improve the modelâ€™s
generalization ability, Noise Robustness add noise to input to make the
model relatively insensitive to small variations in the weights. Early
Stopping and Dropout aims to prevent overfitting by terminate the
leaning or manually ignore certain nodes during learning. Ensemble
methods, such as bagging, could also be implemented to reduce
generalization error.

3. Convolutional Neural Network
============================

Convolutional Neural Networks (ConvNets) are designed to process data
that come in the form of multiple arrays. Many data modalities are in
the form of multiple arrays: 1D for signals and sequences, including
language; 2D for images or audio spectrograms; and 3D for video or
volumetric images. There are four key ideas behind ConvNets that take
advantage of the properties of natural signals: local connections,
shared weights, pooling and the use of many layers. The architecture of
a typical ConvNet is structured as a series of stages. The first few
stages are composed of two types of layers: convolutional layers and
pooling layers. Units in a convolutional layer are organized in feature
maps, within which each unit is connected to local patches in the
feature maps of the previous layer through a set of weights called a
filter bank. The result of this local weighted sum is then passed
through a non-linearity such as a ReLU. All units in a feature map share
the same filter bank. Different feature maps in a layer use different
filter banks.

The reason for this architecture is twofold. First, in array data such
as images, local groups of values are often highly correlated, forming
distinctive local motifs that are easily detected. Second, the local
statistics of images and other signals are invariant to location. In
other words, if a motif can appear in one part of the image, it could
appear anywhere, hence the idea of units at different locations sharing
the same weights and detecting the same pattern in different parts of
the array. The role of the pooling layer is to merge semantically
similar features into one. Because the relative positions of the
features forming a motif can vary somewhat, reliably detecting the motif
can be done by coarse-graining the position of each feature.

Some of the drawbacks of ConvNets also come from the fact that ConvNets
could extract features that represent distinctive local motifs that are
invariant to location. That is part of the reason why ConvNets could be
easily fooled. For example, if you replace the locations of the right
eye with a mouth on the portrait, the ConvNets would still classified
object on this image as a person. Certain methods are created to take
into consideration the spatial relationship of features, such as Capsule
Networks

4. Recurrent Neural Network
========================

Recurrent Neural Network (RNNs) are designed to process sequential data.
RNNs process an input sequence one element at a time, maintaining in
their hidden units a state vector that implicitly contains information
about the history of all the past elements of the sequence. The equation
of a neuron in RNNs is defined as $ h_t = f_w(h_{t-1}, x_t)$, where
$h_t$ is the new state, $h_{t-1}$ is the old state, $x_t$ is the input
vector at some time step, $f_w$ is some function with parameters $w$.
The same function and the same set of parameters are used at every time
step.

RNNs are very powerful dynamic systems, but training them has proved to
be problematic because the backpropagated gradients either grow or
shrink at each time step, so over many time steps they typically explode
or vanish. For exploding problem, we could use Truncated BackPropagation
Through Time method to clip gradients at a threshold or use RMSProp to
adjust the learning rate. For vanishing problem, we could use ReLU or
RMSProp, or Long Shot-Term Memory (LSTM). LSTMs are a special subset of
RNNs that are able to deal with remembering information for much longer
periods of time. Rather than each hidden node being simply a node with a
single activation function, each node is a memory cell that can store
other information. Specifically, it maintains its own cell state.
Vanilla RNNs take in their previous hidden state and the current input
and output a new hidden state. An LSTM does the same, except it also
takes in its old cell state and outputs its new cell state.

5. Generative Models
=================

Generative Models (GMs) belong to unsupervised learning. The goal of GMs
is to generate new samples from the same distribution. The most popular
methods of GMs include Autoencoder and Generative Adversarial Networks
(GANs). The aim of an autoencoder is to learn a representation
(encoding) for a set of data, typically for dimensionality reduction.
GANs implemented by a system of two neural networks contesting with each
other in a zero-sum game framework.

Reference
=================
https://www.nature.com/articles/nature14539
